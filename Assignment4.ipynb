{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW4.0 Final Project description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Project Proposal\n",
    "\n",
    "Intelligent Tutoring Systems are helping saving millions of hours for students and helping them learn deeper. Students imbibe concepts at varying rates of learning. By predicting student performance on problems, content can be tailored to their needs. Target has trainings around the year, which are often limited by its class size. Instead, if these trainings are delivered online adaptive to the learning ability of each employee, we can create a more skilled work force and also build a knowledge repository. The aim of the project is to predict student performance from the log of online trainings for algebra (KDD Cup 2010, Educational Data Mining Challenge. \n",
    "\n",
    "##### Data Source and Description \n",
    "\n",
    "The data is from the KDD cup. Four key terms form the building blocks of the data. These are problem (tasks for the student), step (observable part of the solution), knowledge component (concept required for a step), and opportunity (chance to demonstrate learning a knowledge component).\n",
    "\n",
    "##### Steps\n",
    "\n",
    "1. Data and Problem Understanding\n",
    "2. Build baseline model on sample \n",
    "3. EDA\n",
    "4. Build model on training set \n",
    "5. Test model on a hold out set of the data\n",
    "\n",
    "##### Success metrics Primary- \n",
    "\n",
    "Prediction accuracy of student performance- (AUC for this binary classification task) Secondary- Similarity measures for knowledge components, Time saved in hours \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW4.1 Build a decision to predict whether you can play tennis or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### HW4.1.1 What is the classification accuracy of the tree on the training data?\n",
    "\n",
    "###### Ans : \n",
    "The classification accuracy of a tree on a training dataset will be 100% because an ID3 algorithm will keep on splitting the tree till it reaches the maximum number of layers or till it runs out of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HW4.1.2 Is it possible to produce some set of correct training examples that will get the algorihtm to include the attribute Temperature in the learned tree, even though the true target concept is independent of Temperature? if no, explain. If yes, give such a set.\n",
    "\n",
    "##### Ans. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### HW4.1.3 Now, build a tree using only examples D1–D7. What is the classification accuracy for the training set? what is the accuracy for the test set (examples D8–D14)? explain why you think these are the results.\n",
    "\n",
    "###### Ans. \n",
    "The accuracy in the training dataset (D1-D7) is 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HW4.1.4 In this case, and others, there are only a few labelled examples available for training (that is, no additional data is available for testing or validation). Suggest a concrete pruning strategy, that can be readily embedded in the algorithm, to avoid over fitting. Explain why you think this strategy should work.\n",
    "\n",
    "##### Ans. \n",
    "overfitting can be avoided using decision tree pruning or by limiting the depth of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees for Classification ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset.txt\n",
    "d1 sunny hot high FALSE no\n",
    "d2 sunny hot high TRUE no\n",
    "d3 overcast hot high FALSE yes\n",
    "d4 rainy mild high FALSE yes\n",
    "d5 rainy cool normal FALSE yes\n",
    "d6 rainy cool normal TRUE no\n",
    "d6 overcast cool normal TRUE yes\n",
    "d7 sunny mild high FALSE no\n",
    "d8 sunny cool normal FALSE yes\n",
    "d9 rainy mild normal FALSE yes\n",
    "d10 sunny mild normal TRUE yes\n",
    "d11 overcast mild high TRUE yes\n",
    "d12 overcast hot normal FALSE yes\n",
    "d12 rainy mild high TRUE no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open(\"/Users/z00193k/Desktop/DataScienceTrainingDocs/Assignments/DecisionTree/dataset.txt\", \"r\")\n",
    "inputs=[]\n",
    "for line in f:\n",
    "    inter=[]\n",
    "    current={}\n",
    "    splitline=[]\n",
    "    splitline = line.split()\n",
    "    current[\"outlook\"]=splitline[1]\n",
    "    current[\"temperature\"]=splitline[2]\n",
    "    current[\"humidity\"]=splitline[3]\n",
    "    current[\"wind\"]=splitline[4]\n",
    "    inter.append(current)\n",
    "    inter.append(splitline[5])\n",
    "    inter=tuple(inter)\n",
    "    inputs.append(inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculating the entropy of the data\n",
    "\n",
    "from __future__ import division\n",
    "from collections import Counter, defaultdict\n",
    "from functools import partial\n",
    "import math, random\n",
    "\n",
    "def entropy(class_probabilities):\n",
    "    \"\"\"given a list of class probabilities, compute the entropy\"\"\"\n",
    "    return sum(-p * math.log(p, 2) for p in class_probabilities if p)\n",
    "\n",
    "def class_probabilities(labels):\n",
    "    total_count = len(labels)\n",
    "    return [count/total_count for count in Counter(labels).values()]\n",
    "\n",
    "#This function will return the entropy of input dataset\n",
    "def data_entropy(labeled_data):        \n",
    "    labels = [label for _, label in labeled_data]\n",
    "    probabilities = class_probabilities(labels)\n",
    "    return entropy(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9402859586706309"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_entropy(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy of a Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition_entropy(subsets):\n",
    "    \"\"\"find the entropy from this partition of data into subsets\"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)    \n",
    "    return sum(data_entropy(subset)*len(subset)/total_count for subset in subsets )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_by(items, key_fn):\n",
    "    \"\"\"returns a defaultdict(list), where each input item \n",
    "    is in the list whose key is key_fn(item)\"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for item in items:\n",
    "        key = key_fn(item)\n",
    "        groups[key].append(item)\n",
    "    return groups\n",
    "    \n",
    "def partition_by(inputs, attribute):\n",
    "    \"\"\"returns a dict of inputs partitioned by the attribute\n",
    "    each input is a pair (attribute_dict, label)\"\"\"\n",
    "    return group_by(inputs, lambda x: x[0][attribute])\n",
    "\n",
    "def partition_entropy_by(inputs, attribute):\n",
    "    \"\"\"computes the entropy corresponding to the given partition\"\"\"        \n",
    "    partitions = partition_by(inputs, attribute)\n",
    "    return partition_entropy(partitions.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlook 0.693536138896\n",
      "temperature 0.911063393012\n",
      "humidity 0.788450457308\n",
      "wind 0.892158928262\n"
     ]
    }
   ],
   "source": [
    "for key in['outlook','temperature','humidity','wind']:\n",
    "    print key, partition_entropy_by(inputs, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlook 0.246749819774\n",
      "temperature 0.029222565659\n",
      "humidity 0.151835501362\n",
      "wind 0.0481270304083\n"
     ]
    }
   ],
   "source": [
    "attributes = ['outlook','temperature','humidity','wind']\n",
    "def inf_gain(inputs, keys):\n",
    "    for key in keys:\n",
    "        IG=data_entropy(inputs)-partition_entropy_by(inputs, key)\n",
    "        print key, IG\n",
    "\n",
    "inf_gain(inputs, attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(tree, input):\n",
    "    \"\"\"classify the input using the given decision tree\"\"\"\n",
    "    \n",
    "    # if this is a leaf node, return its value\n",
    "    if tree in [True, False]:\n",
    "        return tree\n",
    "   \n",
    "    # otherwise find the correct subtree\n",
    "    attribute, subtree_dict = tree\n",
    "    \n",
    "    subtree_key = input.get(attribute)  # None if input is missing attribute\n",
    "\n",
    "    if subtree_key not in subtree_dict: # if no subtree for key,\n",
    "        subtree_key = None              # we'll use the None subtree\n",
    "    \n",
    "    subtree = subtree_dict[subtree_key] # choose the appropriate subtree\n",
    "    return classify(subtree, input)     # and use it to classify the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building the tree\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def build_tree_id3(inputs, split_candidates=None):\n",
    "\n",
    "    # if this is our first pass, \n",
    "    # all keys of the first input are split candidates\n",
    "    if split_candidates is None:\n",
    "        split_candidates = inputs[0][0].keys()\n",
    "\n",
    "    # count Trues and Falses in the inputs\n",
    "    num_inputs = len(inputs)\n",
    "    num_trues = len([label for item, label in inputs if label])\n",
    "    num_falses = num_inputs - num_trues\n",
    "    \n",
    "    if num_trues == 0:                  # if only Falses are left\n",
    "        return False                    # return a \"False\" leaf\n",
    "        \n",
    "    if num_falses == 0:                 # if only Trues are left\n",
    "        return True                     # return a \"True\" leaf\n",
    "\n",
    "    if not split_candidates:            # if no split candidates left\n",
    "        return num_trues >= num_falses  # return the majority leaf\n",
    "                            \n",
    "    # otherwise, split on the best attribute\n",
    "    best_attribute = min(split_candidates,\n",
    "        key=partial(partition_entropy_by, inputs))\n",
    "\n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_candidates = [a for a in split_candidates \n",
    "                      if a != best_attribute]\n",
    "    \n",
    "    # recursively build the subtrees\n",
    "    subtrees = { attribute : build_tree_id3(subset, new_candidates)\n",
    "                 for attribute, subset in partitions.iteritems() }\n",
    "\n",
    "    subtrees[None] = num_trues > num_falses # default case\n",
    "\n",
    "    return (best_attribute, subtrees)\n",
    "\n",
    "print \"building the tree\"\n",
    "tree = build_tree_id3(inputs)\n",
    "print tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
