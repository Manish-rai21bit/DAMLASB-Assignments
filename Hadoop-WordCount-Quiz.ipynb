{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAMLAS - Machine Learning At Scale\n",
    "## Assignment - HW2\n",
    "Data Analytics and Machine Learning at Scale\n",
    "Target, Bangalore\n",
    "\n",
    "---\n",
    "__Name:__  *Your Name Goes Here*   \n",
    "__Class:__ DAMLAS (Section *Your Section Goes Here*, e.g., Fall 2016)     \n",
    "__Email:__  *Your Target Email User Goes Here*@Target.com     \n",
    "__Week:__   01\n",
    "\n",
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [HW Introduction](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW  Problems](#3)   \n",
    "    1.0.  [HW2.1](#1.1)   \n",
    "    1.2.  [HW2.2](#1.2)   \n",
    "    1.3.  [HW2.3](#1.3)    \n",
    "    1.4.  [HW2.4](#1.4)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"1\">\n",
    "# 1 Instructions\n",
    "[Back to Table of Contents](#TOC)\n",
    "* Homework submissions are due by Wednesday, 08/30/2016 at 10:30AM.\n",
    "* Word count in Hadoop Streaming with Hadoop is described (should work on your Virtual Machines)\n",
    "\n",
    "\n",
    "* Prepare a single Jupyter notebook (not a requirment), please include questions, and question numbers in the questions and in the responses.\n",
    "Submit your homework notebook via the following form:\n",
    "\n",
    "   + [Submission Link - Google Form](http://goo.gl/forms/er3OFr5eCMWDngB72)\n",
    "\n",
    "\n",
    "### Documents:\n",
    "* IPython Notebook, published and viewable online.\n",
    "* PDF export of IPython Notebook.\n",
    "\n",
    "<a name=\"2\">\n",
    "# 2 Useful References\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "* [Hadoop Primer](https://www.dropbox.com/s/g84vzatuyalhe9y/Lecture%203-Hadoop-Intro-2016-06-13.pdf?dl=0)\n",
    "* [Word Count example using Hadoop Streaming and  Python](http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/\n",
    ")\n",
    "* [Everything you want to know about dictionaries in Python](http://openbookproject.net/thinkcs/python/english3e/dictionaries.html)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Streaming\n",
    "Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer. For example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$HADOOP_HOME/bin/hadoop  jar $HADOOP_HOME/hadoop-streaming.jar \\\n",
    "    -input myInputDirs \\\n",
    "    -output myOutputDir \\\n",
    "    -mapper /bin/cat \\\n",
    "    -reducer /bin/wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Streaming Works\n",
    "\n",
    "In the above example, both the mapper and the reducer are executables that read the input from stdin (line by line) and emit the output to stdout. The utility will create a Map/Reduce job, submit the job to an appropriate cluster, and monitor the progress of the job until it completes.\n",
    "\n",
    "When an executable is specified for mappers, each mapper task will launch the executable as a separate process when the mapper is initialized. As the mapper task runs, it converts its inputs into lines and feed the lines to the stdin of the process. In the meantime, the mapper collects the line oriented outputs from the stdout of the process and converts each line into a key/value pair, which is collected as the output of the mapper. By default, the prefix of a line up to the first tab character is the key and the rest of the line (excluding the tab character) will be the value. If there is no tab character in the line, then entire line is considered as key and the value is null. However, this can be customized, as discussed later.\n",
    "\n",
    "When an executable is specified for reducers, each reducer task will launch the executable as a separate process then the reducer is initialized. As the reducer task runs, it converts its input key/values pairs into lines and feeds the lines to the stdin of the process. In the meantime, the reducer collects the line oriented outputs from the stdout of the process, converts each line into a key/value pair, which is collected as the output of the reducer. By default, the prefix of a line up to the first tab character is the key and the rest of the line (excluding the tab character) is the value. However, this can be customized, as discussed later.\n",
    "\n",
    "This is the basis for the communication protocol between the Map/Reduce framework and the streaming mapper/reducer.\n",
    "\n",
    "## References for Hadoop Streaming\n",
    "* [Hadoop Streaming manual](https://hadoop.apache.org/docs/r1.2.1/streaming.html#Hadoop+Streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of a Hadoop Job\n",
    "\n",
    "For more details see:\n",
    "\n",
    "http://ercoppa.github.io/HadoopInternals/AnatomyMapReduceJob.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export PATH=\"/home/username/anaconda/bin:$PATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## locate the hadoop streaming jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "! find / -name \"hadoop*streaming*\" -print 1> tmp.hadoopJar 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {\"a\":1, \"b\":2, \"d\":4, \"c\":3}          \n",
    "sorted([(value,key) for (key,value) in mydict.items()])\n",
    "\n",
    "#[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jshanahan/Google Drive/JShanahan/Publications/Conferences/STRATA-2012/tutorial_large_scale_mining/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar\r\n",
      "/Users/jshanahan/Publications/Conferences/STRATA-2012/tutorial_large_scale_mining/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar\r\n",
      "/usr/local/Cellar/hadoop/2.6.0/libexec/share/doc/hadoop/hadoop-streaming\r\n",
      "/usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar\r\n",
      "/usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/sources/hadoop-streaming-2.6.0-sources.jar\r\n",
      "/usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/sources/hadoop-streaming-2.6.0-test-sources.jar\r\n"
     ]
    }
   ],
   "source": [
    "!cat tmp.hadoopJar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 107707 Jun 16 13:00 /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing WordCount/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "#sys.stderr.write(\"reporter:counter:Tokens,Total,1\") # NOTE missing the carriage return so wont work\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing my message...how are  you\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    for word in line.split():\n",
    "        print '%s\\t%s' % (word, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing WordCount/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'WordCount/reducer.py #Load code into the current frontend' was not found in history, as a file, url, nor in the user namespace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4292072168fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'load WordCount/reducer.py  #Load code into the current frontend'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jshanahan/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2161\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2165\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jshanahan/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2082\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2084\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2085\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-45>\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, arg_s)\u001b[0m\n",
      "\u001b[0;32m/Users/jshanahan/anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jshanahan/anaconda/lib/python2.7/site-packages/IPython/core/magics/code.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0msearch_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'n'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_user_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_ns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m's'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jshanahan/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mfind_user_code\u001b[0;34m(self, target, raw, py_only, skip_encoding_cookie, search_ns)\u001b[0m\n\u001b[1;32m   3184\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3185\u001b[0m             raise ValueError((\"'%s' was not found in history, as a file, url, \"\n\u001b[0;32m-> 3186\u001b[0;31m                                 \"nor in the user namespace.\") % target)\n\u001b[0m\u001b[1;32m   3187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodeobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'WordCount/reducer.py #Load code into the current frontend' was not found in history, as a file, url, nor in the user namespace."
     ]
    }
   ],
   "source": [
    "%load WordCount/reducer.py  #Load code into the current frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x WordCount/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: WordCount/mapper.py: Permission denied\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:processing my message...how are you\r\n",
      "reporter:counter:Reducer Counters,Calls,1\r\n",
      "foo\t3\r\n",
      "quux\t2\r\n",
      "bar\t1\r\n",
      "labs\t1\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py | sort -k1,1 | WordCount/reducer.py| sort -k2,2nr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] [-v] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] <path> ...]\n",
      "\t[-expunge]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] <src> <localdst>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-d] [-h] [-R] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] <file>]\n",
      "\t[-test -[defsz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are\n",
      "-conf <configuration file>     specify an application configuration file\n",
      "-D <property=value>            use value for given property\n",
      "-fs <local|namenode:port>      specify a namenode\n",
      "-jt <local|resourcemanager:port>    specify a ResourceManager\n",
      "-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n",
      "-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n",
      "-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n",
      "\n",
      "The general command line syntax is\n",
      "bin/hadoop command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting testWordCountInput.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile testWordCountInput.txt\n",
    "hello this is Jimi\n",
    "jimi who Jimi three Jimi \n",
    "Hello\n",
    "hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cd WordCount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted testWordCountInput.txt\n",
      "Deleted wordcount-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob7511691146345687632.jar tmpDir=null\n",
      "16/08/26 20:46:58 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/08/26 20:46:59 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/08/26 20:47:00 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/08/26 20:47:00 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/08/26 20:47:00 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/08/26 20:47:00 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/08/26 20:47:00 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/08/26 20:47:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1472236879328_0004\n",
      "16/08/26 20:47:01 INFO impl.YarnClientImpl: Submitted application application_1472236879328_0004\n",
      "16/08/26 20:47:01 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1472236879328_0004/\n",
      "16/08/26 20:47:01 INFO mapreduce.Job: Running job: job_1472236879328_0004\n",
      "16/08/26 20:47:11 INFO mapreduce.Job: Job job_1472236879328_0004 running in uber mode : false\n",
      "16/08/26 20:47:11 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/08/26 20:47:24 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/08/26 20:47:25 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/08/26 20:47:40 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "16/08/26 20:47:43 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/08/26 20:47:44 INFO mapreduce.Job: Job job_1472236879328_0004 completed successfully\n",
      "16/08/26 20:47:44 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=100\n",
      "\t\tFILE: Number of bytes written=594849\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=324\n",
      "\t\tHDFS: Number of bytes written=56\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=22248\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=43543\n",
      "\t\tTotal time spent by all map tasks (ms)=22248\n",
      "\t\tTotal time spent by all reduce tasks (ms)=43543\n",
      "\t\tTotal vcore-seconds taken by all map tasks=22248\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=43543\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=22781952\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=44588032\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=11\n",
      "\t\tMap output bytes=78\n",
      "\t\tMap output materialized bytes=118\n",
      "\t\tInput split bytes=240\n",
      "\t\tCombine input records=11\n",
      "\t\tCombine output records=9\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=118\n",
      "\t\tReduce input records=9\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=18\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=492\n",
      "\t\tCPU time spent (ms)=3990\n",
      "\t\tPhysical memory (bytes) snapshot=810594304\n",
      "\t\tVirtual memory (bytes) snapshot=7526408192\n",
      "\t\tTotal committed heap usage (bytes)=513482752\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=8\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=84\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=56\n",
      "16/08/26 20:47:44 INFO streaming.StreamJob: Output directory: wordcount-output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm testWordCountInput.txt \n",
    "!hdfs dfs -copyFromLocal testWordCountInput.txt \n",
    "!hdfs dfs -rm -r wordcount-output\n",
    "#usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib\n",
    "dataDir = \"/Users/jshanahan/Dropbox/lectures-uc-berkeley-ml-class-2015/Notebooks/WordCount\"\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "   -mapper /home/cloudera/WordCount/mapper.py \\\n",
    "   -reducer /home/cloudera/WordCount/reducer.py \\\n",
    "   -combiner /home/cloudera/WordCount/reducer.py \\\n",
    "   -input testWordCountInput.txt \\\n",
    "   -output wordcount-output  \\\n",
    "   -numReduceTasks 3\n",
    "   #--D mapreduce.job.reduces=2  depecated\n",
    "#-input historical_tours.txt  file on Hadoop\n",
    "\n",
    "\n",
    "#output directory on Hadoop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n---------------------------\\n\n",
      "hello this is Jimi\n",
      "jimi who Jimi three Jimi \n",
      "Hello\n",
      "hello\\n---------------------------\\n\n",
      "Hello\t1\n",
      "jimi\t1\n",
      "this\t1\n",
      "three\t1\n",
      "Jimi\t3\n",
      "hello\t2\n",
      "is\t1\n",
      "who\t1\n"
     ]
    }
   ],
   "source": [
    "#have a look at the input\n",
    "!echo  \"\\n---------------------------\\n\"\n",
    "!hdfs dfs -cat testWordCountInput.txt\n",
    "!echo  \"\\n---------------------------\\n\"\n",
    "# Wordcount output\n",
    "!hdfs dfs -cat wordcount-output/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  HW1  <a name=\"3\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "* Dont forget to create a new subdirectory for this homework (on your local machine/virtual machine, and on HDFS)\n",
    "---\n",
    "## 3.  HW2.1  <a name=\"1.1\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Change the mapper.py/reducer.py combination from the the above WordCount example so that you get the longest word present in the text version of Aliceâ€™s Adventures in Wonderland. (You can obtain a free plain text version of the book, along with many others, from [here](http://www.gutenberg.org.). First use one reducer and report your result. HINT: from emit records of the form: \"longestWord\\theLongWordEver\\t15\".  What happens if you use 3 reducers? Anything change with respect to your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!curl 'http://www.gutenberg.org/cache/epub/11/pg11.txt' -o alicesTExtFilename.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW2.2  <a name=\"1.1\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Change the mapper.py/reducer.py combination so that you get only the number of words starting with an uppercase letter, and the number of words starting with a lowercase letter. In other words, you need an output file with only 2 lines, one giving you the number of words staring with a lowercase ('a' to 'z'), and the other line indicating the number of words starting with an uppercase letter ('A' to 'Z'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW2.3  <a name=\"1.1\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "The data file at http://www.metoffice.gov.uk/climate/uk/stationdata/heathrowdata.txt (mirrored here) is part of a set of text files cataloguing temperatures collected in different cities of the UK. The one for the URL given is for Heathrow (London Airport). Typically each city is held in one data file. You can access the whole set from [here](http://www.metoffice.gov.uk/climate/uk/stationdata/).\n",
    "Write a MapReduce application with a mapper.py and reduce.py that uses only the text file for Heathrow as input and that outputs the lowest temperature ever recorded and the year this event happened. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW2.4  <a name=\"1.1\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Reproduce the following webpage (Word Count in Hadoop Streaming over different examples) in a Jupyter notebook:\n",
    "\n",
    "* http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This call to Hadoop does NOT work as anticpated\n",
    "# IdentityMapper, IdentityReducer do not trigger the Hadoop framework to sort properly\n",
    "\n",
    "!hdfs dfs -rm -r wordcount-output-sorted\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "   -input wordcount-output \\\n",
    "   -output wordcount-output-sorted  \\\n",
    "   -numReduceTasks 1 \\\n",
    "   -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "   -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n",
    "\n",
    "#DOES not work in streaming mode\n",
    "#   -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "#  -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted wordcount-output-sorted\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob9044814104207183172.jar tmpDir=null\n",
      "16/08/26 21:29:59 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/08/26 21:29:59 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/08/26 21:30:01 INFO mapred.FileInputFormat: Total input paths to process : 3\n",
      "16/08/26 21:30:01 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/08/26 21:30:01 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/08/26 21:30:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1472236879328_0006\n",
      "16/08/26 21:30:02 INFO impl.YarnClientImpl: Submitted application application_1472236879328_0006\n",
      "16/08/26 21:30:03 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1472236879328_0006/\n",
      "16/08/26 21:30:03 INFO mapreduce.Job: Running job: job_1472236879328_0006\n",
      "16/08/26 21:30:18 INFO mapreduce.Job: Job job_1472236879328_0006 running in uber mode : false\n",
      "16/08/26 21:30:18 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/08/26 21:30:43 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/08/26 21:30:45 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/08/26 21:31:08 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "16/08/26 21:31:09 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/08/26 21:31:09 INFO mapreduce.Job: Job job_1472236879328_0006 completed successfully\n",
      "16/08/26 21:31:09 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=98\n",
      "\t\tFILE: Number of bytes written=712753\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=431\n",
      "\t\tHDFS: Number of bytes written=64\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=65014\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=66700\n",
      "\t\tTotal time spent by all map tasks (ms)=65014\n",
      "\t\tTotal time spent by all reduce tasks (ms)=66700\n",
      "\t\tTotal vcore-seconds taken by all map tasks=65014\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=66700\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=66574336\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=68300800\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8\n",
      "\t\tMap output records=8\n",
      "\t\tMap output bytes=64\n",
      "\t\tMap output materialized bytes=134\n",
      "\t\tInput split bytes=375\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=134\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=16\n",
      "\t\tShuffled Maps =9\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=9\n",
      "\t\tGC time elapsed (ms)=1191\n",
      "\t\tCPU time spent (ms)=3780\n",
      "\t\tPhysical memory (bytes) snapshot=983003136\n",
      "\t\tVirtual memory (bytes) snapshot=9025863680\n",
      "\t\tTotal committed heap usage (bytes)=679096320\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=56\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=64\n",
      "16/08/26 21:31:09 INFO streaming.StreamJob: Output directory: wordcount-output-sorted\n"
     ]
    }
   ],
   "source": [
    "#Custom partitioner works \n",
    "# partition based on first parts\n",
    "#sort numerically decreasing on the third part\n",
    "!hdfs dfs -rm -r wordcount-output-sorted \n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "  -D stream.num.map.output.key.fields=4 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "  -D mapreduce.job.reduces=3 \\\n",
    "  -input wordcount-output \\\n",
    "  -output wordcount-output-sorted \\\n",
    "    -mapper /bin/cat \\\n",
    "   -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner\n",
    "    \n",
    "#   -mapper identityMapper.py \\\n",
    "\n",
    "#Can also use -mapper /bin/cat \\\n",
    "#-mapper /bin/cat \\\n",
    "#-reducer /bin/cat \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 cloudera cloudera          0 2016-08-26 21:31 wordcount-output-sorted/part-00000\n",
      "-rw-r--r--   1 cloudera cloudera         42 2016-08-26 21:31 wordcount-output-sorted/part-00001\n",
      "-rw-r--r--   1 cloudera cloudera         22 2016-08-26 21:31 wordcount-output-sorted/part-00002\n",
      "Jimi\t3\t\n",
      "Hello\t1\t\n",
      "jimi\t1\t\n",
      "this\t1\t\n",
      "three\t1\t\n",
      "hello\t2\t\n",
      "is\t1\t\n",
      "who\t1\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls wordcount-output-sorted/part* \n",
    "!hdfs dfs -cat wordcount-output-sorted/part* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
