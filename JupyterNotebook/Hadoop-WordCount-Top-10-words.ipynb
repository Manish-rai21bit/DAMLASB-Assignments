{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAMLAS - Machine Learning At Scale\n",
    "## Assignment - HW3\n",
    "Data Analytics and Machine Learning at Scale\n",
    "Target, Bangalore\n",
    "\n",
    "---\n",
    "__Name:__  *Your Name Goes Here*   \n",
    "__Class:__ DAMLAS (Section *Your Section Goes Here*, e.g., Fall 2016)     \n",
    "__Email:__  *Your Target Email User Goes Here*@Target.com     \n",
    "__Week:__   01\n",
    "\n",
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [HW Introduction](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW  Problems](#3)   \n",
    "    1.0.  [HW3.1](#1.1)   \n",
    "    1.2.  [HW3.2](#1.2)   \n",
    "    1.3.  [HW3.3](#1.3)    \n",
    "    1.4.  [HW3.4](#1.4)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"1\">\n",
    "# 1 Instructions\n",
    "[Back to Table of Contents](#TOC)\n",
    "* Homework submissions are due by Thursday, 08/30/2016 at 10:30AM.\n",
    "\n",
    "\n",
    "* Prepare a single Jupyter notebook (not a requirment), please include questions, and question numbers in the questions and in the responses.\n",
    "Submit your homework notebook via the following form:\n",
    "\n",
    "   + [Submission Link - Google Form](http://goo.gl/forms/er3OFr5eCMWDngB72)\n",
    "\n",
    "\n",
    "### Documents:\n",
    "* IPython Notebook, published and viewable online.\n",
    "* PDF export of IPython Notebook.\n",
    "\n",
    "<a name=\"2\">\n",
    "# 2 Useful References\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "* [Total Sort Notebook](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/44idk0xz2wyu45l/total-sort-guide.ipynb) \n",
    "* [Hadoop Primer](https://www.dropbox.com/s/g84vzatuyalhe9y/Lecture%203-Hadoop-Intro-2016-06-13.pdf?dl=0)\n",
    "* [Word Count example using Hadoop Streaming and  Python](http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/\n",
    ")\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export PATH=\"/home/username/anaconda/bin:$PATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## locate the hadoop streaming jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "! find / -name \"hadoop*streaming*\" -print 1> tmp.hadoopJar 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jshanahan/Google Drive/JShanahan/Publications/Conferences/STRATA-2012/tutorial_large_scale_mining/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar\r\n",
      "/Users/jshanahan/Publications/Conferences/STRATA-2012/tutorial_large_scale_mining/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar\r\n",
      "/usr/local/Cellar/hadoop/2.6.0/libexec/share/doc/hadoop/hadoop-streaming\r\n",
      "/usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar\r\n",
      "/usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/sources/hadoop-streaming-2.6.0-sources.jar\r\n",
      "/usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/sources/hadoop-streaming-2.6.0-test-sources.jar\r\n"
     ]
    }
   ],
   "source": [
    "!cat tmp.hadoopJar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 107707 Jun 16 13:00 /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing WordCount/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "#sys.stderr.write(\"reporter:counter:Tokens,Total,1\") # NOTE missing the carriage return so wont work\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing my message...how are  you\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    for word in line.split():\n",
    "        print '%s\\t%s' % (word, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing WordCount/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x WordCount/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: WordCount/mapper.py: Permission denied\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:processing my message...how are you\r\n",
      "reporter:counter:Reducer Counters,Calls,1\r\n",
      "foo\t3\r\n",
      "quux\t2\r\n",
      "bar\t1\r\n",
      "labs\t1\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py | sort -k1,1 | WordCount/reducer.py| sort -k2,2nr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting testWordCountInput.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile testWordCountInput.txt\n",
    "hello this is Jimi\n",
    "jimi who Jimi three Jimi \n",
    "Hello\n",
    "hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cd WordCount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted testWordCountInput.txt\n",
      "Deleted wordcount-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob7511691146345687632.jar tmpDir=null\n",
      "16/08/26 20:46:58 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/08/26 20:46:59 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/08/26 20:47:00 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/08/26 20:47:00 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/08/26 20:47:00 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/08/26 20:47:00 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/08/26 20:47:00 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/08/26 20:47:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1472236879328_0004\n",
      "16/08/26 20:47:01 INFO impl.YarnClientImpl: Submitted application application_1472236879328_0004\n",
      "16/08/26 20:47:01 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1472236879328_0004/\n",
      "16/08/26 20:47:01 INFO mapreduce.Job: Running job: job_1472236879328_0004\n",
      "16/08/26 20:47:11 INFO mapreduce.Job: Job job_1472236879328_0004 running in uber mode : false\n",
      "16/08/26 20:47:11 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/08/26 20:47:24 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/08/26 20:47:25 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/08/26 20:47:40 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "16/08/26 20:47:43 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/08/26 20:47:44 INFO mapreduce.Job: Job job_1472236879328_0004 completed successfully\n",
      "16/08/26 20:47:44 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=100\n",
      "\t\tFILE: Number of bytes written=594849\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=324\n",
      "\t\tHDFS: Number of bytes written=56\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=22248\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=43543\n",
      "\t\tTotal time spent by all map tasks (ms)=22248\n",
      "\t\tTotal time spent by all reduce tasks (ms)=43543\n",
      "\t\tTotal vcore-seconds taken by all map tasks=22248\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=43543\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=22781952\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=44588032\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=11\n",
      "\t\tMap output bytes=78\n",
      "\t\tMap output materialized bytes=118\n",
      "\t\tInput split bytes=240\n",
      "\t\tCombine input records=11\n",
      "\t\tCombine output records=9\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=118\n",
      "\t\tReduce input records=9\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=18\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=492\n",
      "\t\tCPU time spent (ms)=3990\n",
      "\t\tPhysical memory (bytes) snapshot=810594304\n",
      "\t\tVirtual memory (bytes) snapshot=7526408192\n",
      "\t\tTotal committed heap usage (bytes)=513482752\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=8\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=84\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=56\n",
      "16/08/26 20:47:44 INFO streaming.StreamJob: Output directory: wordcount-output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm testWordCountInput.txt \n",
    "!hdfs dfs -copyFromLocal testWordCountInput.txt \n",
    "!hdfs dfs -rm -r wordcount-output\n",
    "#usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib\n",
    "dataDir = \"/Users/jshanahan/Dropbox/lectures-uc-berkeley-ml-class-2015/Notebooks/WordCount\"\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "   -mapper /home/cloudera/WordCount/mapper.py \\\n",
    "   -reducer /home/cloudera/WordCount/reducer.py \\\n",
    "   -combiner /home/cloudera/WordCount/reducer.py \\\n",
    "   -input testWordCountInput.txt \\\n",
    "   -output wordcount-output  \\\n",
    "   -numReduceTasks 3\n",
    "   #--D mapreduce.job.reduces=2  depecated\n",
    "#-input historical_tours.txt  file on Hadoop\n",
    "\n",
    "\n",
    "#output directory on Hadoop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n---------------------------\\n\n",
      "hello this is Jimi\n",
      "jimi who Jimi three Jimi \n",
      "Hello\n",
      "hello\\n---------------------------\\n\n",
      "Hello\t1\n",
      "jimi\t1\n",
      "this\t1\n",
      "three\t1\n",
      "Jimi\t3\n",
      "hello\t2\n",
      "is\t1\n",
      "who\t1\n"
     ]
    }
   ],
   "source": [
    "#have a look at the input\n",
    "!echo  \"\\n---------------------------\\n\"\n",
    "!hdfs dfs -cat testWordCountInput.txt\n",
    "!echo  \"\\n---------------------------\\n\"\n",
    "# Wordcount output\n",
    "!hdfs dfs -cat wordcount-output/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  HW3  <a name=\"3\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "* Dont forget to create a new subdirectory for this homework (on your local machine/virtual machine, and on HDFS)\n",
    "---\n",
    "## 3.  HW3.1  <a name=\"1.1\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "In Hadoop streaming write code to determing the top-10 most frequent  using  words present in the text version of Alice’s Adventures in Wonderland. (You can obtain a free plain text version of the book, along with many others, from [here](http://www.gutenberg.org.). First use one reducer and report your result. Then develop a solution when you use 3 reducers? \n",
    "\n",
    "\n",
    "* Please leverage the following notebook for this HW problem: [Total Sort Notebook](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/44idk0xz2wyu45l/total-sort-guide.ipynb)\n",
    "\n",
    "For your result please  create a text file named __alice_sorted_words.txt__ containing an  listing of all the words, and the number of times each occurs, in the text version of Alice’s Adventures in Wonderland in the following sort order: WordCount descreasing, and where words that have the same count, they be ordered in alphabetical increasing. (You can obtain a free plain text version of the book, along with many others, from here The first 10 lines of your output file should look something like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Word              Count\n",
    "=======================\n",
    "a                 631\n",
    "a-piece           1\n",
    "abide             1\n",
    "able              1\n",
    "about             94\n",
    "above             3\n",
    "absence           1\n",
    "absurd            2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!curl 'http://www.gutenberg.org/cache/epub/11/pg11.txt' -o alicesTExtFilename.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW3.2  <a name=\"1.2\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "The data file at http://www.metoffice.gov.uk/climate/uk/stationdata/heathrowdata.txt (mirrored here) is part of a set of text files cataloguing temperatures collected in different cities of the UK. The one for the URL given is for Heathrow (London Airport). Typically each city is held in one data file. You can access the whole set from [here](http://www.metoffice.gov.uk/climate/uk/stationdata/).\n",
    "Write a MapReduce application with a mapper.py and reduce.py that uses only the text file for Heathrow as input and that outputs all records in the following order: date increasing and temperature decreasing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
